{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HNfPb3gzHV8",
    "outputId": "f9735e45-09a5-4df3-bd2b-72def28b23e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-18 17:59:50--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-03-18 17:59:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-03-18 17:59:51--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.16MB/s    in 2m 40s  \n",
      "\n",
      "2021-03-18 18:02:31 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "#upload glove.6B.300d.txt\n",
    "import os\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('glove6B.6B.zip', <http.client.HTTPMessage at 0x11d163417f0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "filename = 'glove6B.6B.zip'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "hPpispZEmEBr",
    "outputId": "fedc425b-7b9c-4ee5-cc2d-54e65b0bafdd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-69b3c557-9614-411a-b527-6f698dd3de04\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-69b3c557-9614-411a-b527-6f698dd3de04\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload small.csv file\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "U9vuxbC3neET",
    "outputId": "86147ab9-6397-44ad-9a8b-c2360763f7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   reviewText  99946 non-null   object\n",
      " 1   overall     100000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They look good and stick good! I just don't li...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These stickers work like the review says they ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These are awesome and make my phone look so st...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Item arrived in great time and was in perfect ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome! stays on, and looks great. can be use...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>I ordered the Dr. Green/Solid White version fr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>I bought this cover for my husband's phone and...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>The case has really nice colors, very vibrant....</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>The replacement is on its way. I recommend thi...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>Yet Another Great Case! I Would Rebuy This Cas...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              reviewText  overall\n",
       "0      They look good and stick good! I just don't li...        4\n",
       "1      These stickers work like the review says they ...        5\n",
       "2      These are awesome and make my phone look so st...        5\n",
       "3      Item arrived in great time and was in perfect ...        4\n",
       "4      awesome! stays on, and looks great. can be use...        5\n",
       "...                                                  ...      ...\n",
       "99995  I ordered the Dr. Green/Solid White version fr...        4\n",
       "99996  I bought this cover for my husband's phone and...        5\n",
       "99997  The case has really nice colors, very vibrant....        5\n",
       "99998  The replacement is on its way. I recommend thi...        5\n",
       "99999  Yet Another Great Case! I Would Rebuy This Cas...        5\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(\"small.csv\")\n",
    "test.info()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF7Tt9ANlZ37"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "#the following working directory should contain small.csv and glove.6B.300d.txt \n",
    "os.chdir('/content/drive/My Drive/Teaching/teaching at Lehigh/2021_sp_nlp/Project 1/data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbdCMdr6uxlS"
   },
   "source": [
    "## Text Preprocessing and Dataset Construction\n",
    "\n",
    "### Define the WordIndexer class to\n",
    "*   hold the mapping from words to their indices and the indices to words.\n",
    "*   map from a list of sentences to a list of integers so that words are mapped to their indices, in the same order as the original words (except some words replaced).\n",
    "\n",
    "### Inherit from the `torch.utils.data.Dataset` class and create the AmazonReviewGloveDataset class to\n",
    "\n",
    "\n",
    "*   load the Amazon reviews in the csv format. Tokenize the review texts into sentences (a review can contain more than one sentence).\n",
    "*   use the WordIndexer class to obtain the indices of the words in the sentences.\n",
    "*   compute the X (word co-occurrence) matrix as the Glove paper indicates.\n",
    "\n",
    "We provide the function to read the pretrained word vectors from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFcyggrF0h-6",
    "outputId": "fd1d4a80-e53a-478a-b0e6-73b952fc00c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chase\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import itertools\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class WordIndexer:\n",
    "    \"\"\"Transform a dataset of text to a list of index of words.\"\"\"\n",
    "\n",
    "    def __init__(self, min_word_occurrences=10, oov_word=\"OOV\"):\n",
    "        \"\"\" min_word_occurrences: integer, the minimum frequency of the word to keep.\n",
    "            oov_word: string, a special string for out-of-vocabulary words.\n",
    "        \"\"\"\n",
    "        self.oov_word = oov_word\n",
    "        self.min_word_occurrences = min_word_occurrences\n",
    "        # word to integer index mapping\n",
    "        self.word_to_index = {oov_word: 0}\n",
    "        # the inverse of the above mapping\n",
    "        self.index_to_word = [oov_word]\n",
    "        # this is for storing the word frequencies for removing infrequent words\n",
    "        self.word_occurrences = {}\n",
    "        # regular expression for retaining meaningful English words\n",
    "        self.re_words = re.compile(r\"\\b[a-zA-Z]{2,}\\b\")\n",
    "\n",
    "    def get_word_index(self, word, add_new_word = True):\n",
    "        \"\"\" Find the index of a word.\n",
    "                \n",
    "            word: string, the query word.\n",
    "            add_new_word: if true and the word has no entry, assign a new integer index to word.\n",
    "                            if false, return the index of the oov_word\n",
    "        \"\"\"\n",
    "        ### Your codes go here (10 points) ###  ###COMPLETE###\n",
    "        if word not in self.index_to_word:\n",
    "          if add_new_word == True: \n",
    "            self.index_to_word.append(word)\n",
    "            self.word_to_index[word] = self.index_to_word.index(word)\n",
    "          else:\n",
    "            return -1\n",
    "        else:\n",
    "          return self.index_to_word.index(word)\n",
    "      \n",
    "    @property\n",
    "    def n_words(self):\n",
    "        \"\"\" return: the vocabulary size\n",
    "        \"\"\"\n",
    "        return len(self.word_to_index)\n",
    "\n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\" texts: list of sentences, each of which is a string\n",
    "            \n",
    "            Split each sentence into a list of words.\n",
    "            Then filter out the infrequent words.\n",
    "            Other text preprocessing, such as\n",
    "                lower-casing,\n",
    "                stop-word removal, and\n",
    "                advance word tokenization\n",
    "                are possible here.\n",
    "            Lastly setup the word-to-index and index-to-word dictionaries.\n",
    "            \n",
    "            return: a list of lists of indices of words in each sentence.\n",
    "                    For example: [[1,2,3], [4,5,6]] where,\n",
    "                        [1,2,3] are the indices of words in the first sentence\n",
    "                        [4,5,6] are the indices of words in the second sentence\n",
    "                    \n",
    "        \"\"\"\n",
    "        #STEP 1 fit_transform \n",
    "        #obtain stop words in root folder\n",
    "        ### Your codes go here (10 points) ###\n",
    "        ####COMPLETE#####CAN ADD MORE THINGS TO MAKE WORDS BETTER####\n",
    "\n",
    "        f_stopwords = open(r\"C:\\Users\\chase\\AppData\\roaming\\nltk_data\\corpora\\stopwords\\english\", 'r', encoding=\"utf8\")\n",
    "\n",
    "        #take words from file and sanitize\n",
    "        stopwords_english = []\n",
    "        for line in f_stopwords:\n",
    "          clean_line = line.replace(\"\\n\", \"\")\n",
    "          stopwords_english.append(clean_line)\n",
    "                \n",
    "        words = [] #list of single words to help with counter class (counter doesnt work on list of lists)\n",
    "        list_sentences_to_words = [] #future corupus\n",
    "\n",
    "        print(\"Tokenizing sentences into words...\")\n",
    "        for sentence in texts:\n",
    "          sentence_to_word = nltk.tokenize.word_tokenize(sentence) #convert sentences to list of words\n",
    "          filtered_sentence = []\n",
    "          for word in sentence_to_word: \n",
    "            if word.lower() not in stopwords_english:  #filter sentence of stopwords and convert to lowercase\n",
    "              filtered_sentence.append(word.lower())\n",
    "              words.append(word.lower())  \n",
    "          list_sentences_to_words.append(filtered_sentence)\n",
    "        print(\"Done implementing tokenization and taking out stopwords.\")\n",
    "      \n",
    "          # Step 2 fit_transform: Build a dictionary using the Counter class\n",
    "        # keep the unique words and their counts\n",
    "        # filter out the infrequent ones using the threshold self.min_word_occurrences.\n",
    "        # the results is a vocabulary in self.word_to_index and self.index_to_word.\n",
    "        ### Your codes go here (10 points)\n",
    "         ######COMPLETE#####\n",
    "\n",
    "        print(\"Building a dictionary with occurrences of words...\")\n",
    "        self.word_occurrences = dict(Counter(words)) #get list of occurrences in words list (list of words from sentences) using counter class\n",
    "\n",
    "        print(\"Removing words that don't meet the minimum word occurrences...\")\n",
    "        for word in list(self.word_occurrences):\n",
    "            if self.word_occurrences.get(word) < self.min_word_occurrences:\n",
    "              self.word_occurrences.pop(word)\n",
    "\n",
    "        self.index_to_word.extend(list(self.word_occurrences))\n",
    "\n",
    "        for word in self.index_to_word:\n",
    "            self.word_to_index[word] = self.index_to_word.index(word) #inverse of index_to_word\n",
    "        print(\"Vocabulary in word_to_index and index_to_word.\")\n",
    "\n",
    "\n",
    "        # save the word and their counts to a file.\n",
    "\n",
    "        with open('./train_word_counts.txt', 'w') as out_f:\n",
    "            a = sorted([(word, count) for word, count in self.word_occurrences.items()],\n",
    "                   key = lambda x:x[1], reverse=True)\n",
    "            for word, count in a:\n",
    "                out_f.write('{}:{}\\n'.format(word, count))\n",
    "\n",
    "        # Step 3 fit_transform: build and return the corpus in index representation\n",
    "        # using the vocabulary built in the last step.\n",
    "        # Be careful about words that are not in the vocabulary.\n",
    "        ### Your codes go here (10 points) ###\n",
    "        ###COMPLETE####\n",
    "\n",
    "        corpus = []\n",
    "        for sentence in list_sentences_to_words:\n",
    "            corpus_sentence = []\n",
    "            for word in sentence:\n",
    "              res = self.word_to_index.get(word)\n",
    "              if res is None:\n",
    "                corpus_sentence.append(0)\n",
    "              else:\n",
    "                corpus_sentence.append(res)\n",
    "            corpus.append(corpus_sentence)\n",
    "\n",
    "        return corpus\n",
    "\n",
    "    \n",
    "class AmazonReviewGloveDataset(Dataset):\n",
    "    def __init__(self, path, right_window = 4, min_word_occurrences = 10):        \n",
    "        \"\"\" Load the reviews from a csv file. One row is one review.\n",
    "                \n",
    "            path: path to the csv file containing the reviews and their ratings\n",
    "            right_window: integer, how large the window is to get context words.\n",
    "            min_word_occurrences: integer, the minimum frequency of the word to keep.\n",
    "\n",
    "            No return value\n",
    "        \"\"\"\n",
    "        self.right_window = right_window\n",
    "\n",
    "        # Step 1: tokenize the first field of each row in the csv file into sentences\n",
    "        #         (e.g. using nltk.tokenize.sent_tokenize).\n",
    "        #           Use pandas.read_csv to load the given training csv file.\n",
    "        df = pd.read_csv(path)\n",
    "        texts = []  # each element of texts is a single sentence.\n",
    "        \n",
    "        ### Your codes go here (10 points) ### ###COMPLETE####\n",
    "        df.dropna(inplace=True) #remove NaN from dataframe\n",
    "        for sentence in df['reviewText']:\n",
    "            tokenized_sent = nltk.tokenize.sent_tokenize(sentence) #tokenize each review into sentences\n",
    "            for sent in tokenized_sent: #for each sentence in tokenized_sent, append into texts\n",
    "              texts.append(sent)\n",
    "\n",
    "        print ('{} reviews loaded. {} sentences.'.format(df.shape[0], len(texts)))\n",
    "\n",
    "        \n",
    "        # Step 2: pass the list of all sentences from step 1 (texts) to WordIndexer.\n",
    "        # Use its fit_transform function to turn list of sentences into list of lists of word indices in the sentences.\n",
    "        # Keep the word ordering.\n",
    "        print ('Indexing the corpus...')\n",
    "        self.indexer = WordIndexer(min_word_occurrences=min_word_occurrences)\n",
    "        corpus = self.indexer.fit_transform(texts)\n",
    "        print ('Done indexing the corpus.')\n",
    "        \n",
    "        \n",
    "        # Step 3: go through the results (corpus) from step 2 and gather (center, context) in comatrix,\n",
    "        # which is a collections.Counter object.\n",
    "        # In the Counter, keys are (center, context) pairs\n",
    "        # values are the number of their co-occurrence as defined in the Glove paper.\n",
    "        print ('Constructing the co-occurrence matrix...')\n",
    "        ### Your codes go here (10 points) ###\n",
    "        ####COMPLETE#####\n",
    "        #use itertools to find combinations in corpus\n",
    "        combinations = [tuple(itertools.combinations(word, 2)) for word in corpus]\n",
    "        combinations = itertools.chain(*combinations)\n",
    "        #use counter class to count number of combinations\n",
    "        comatrix = Counter(combinations)\n",
    "        print (\"Done constructing the co-occurrence matrix.\")\n",
    "\n",
    "\n",
    "        # save the comatrix to file                    \n",
    "        with open('./comatrix.pkl', 'wb') as out_f:\n",
    "            pickle.dump(comatrix, out_f)\n",
    "\n",
    "        # Step 4: flatten the co-occurrence matrix and store the center, context, and X_ij\n",
    "        # in three lists: self.left (center word), self.right (context word), self.n_occurrences (X_ij)\n",
    "        self.left, self.right, self.n_occurrences = [], [], [] #include self\n",
    "        ### Your codes go here (10 points) ###\n",
    "        ###COMPLETE#####\n",
    "        comatrix_dict = dict(comatrix)\n",
    "        keys = list(comatrix_dict.keys())\n",
    "        for key in keys:\n",
    "          x, y = key\n",
    "          self.left.append(x)\n",
    "          self.right.append(y)\n",
    "\n",
    "        self.n_occurrences = list(comatrix_dict.values())\n",
    "        print(\"Number of entries = \", len(comatrix)) #more than his (i have more word therefore will be greater), should remove non english words using regex soon. \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.left[index], self.right[index], self.n_occurrences[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.left)\n",
    "    \n",
    "def load_pretrained_wv(path):\n",
    "    \"\"\"\n",
    "        Load the pretrained word vectors downloaded from Stanford NLP.\n",
    "    \"\"\"\n",
    "    wv = {}\n",
    "    with open(path, 'r',encoding='utf8') as f:\n",
    "        for line in f.readlines():\n",
    "            items = line.split(' ')\n",
    "            wv[items[0]] = torch.DoubleTensor([float(a) for a in items[1:]])\n",
    "    return wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smxzeldAJzNX"
   },
   "source": [
    "### Test of WordIndexer and AmazonReviewGloveDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "Lzv3KCwHBrVv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.functional as F\n",
    "\n",
    "class GloveModel(nn.Module):\n",
    "    def __init__(self, word_indexer, wv = None, word_dims = 300, BASE_STD = 0.01, random_state = 0):\n",
    "        \"\"\" Specify and initialize the parameters of the Glove network.\n",
    "        \"\"\"\n",
    "        super(GloveModel, self).__init__()\n",
    "        num_words = word_indexer.n_words\n",
    "        \n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "        # initialize left and right word vectors\n",
    "        self.L_vecs = (torch.randn((num_words, word_dims))  * BASE_STD)\n",
    "        self.R_vecs = (torch.randn((num_words, word_dims))  * BASE_STD)\n",
    "       \n",
    "        if wv is not None:\n",
    "            num_replaced = 0\n",
    "            for i in range(num_words):\n",
    "                word = word_indexer.index_to_word[i]\n",
    "                if word in wv:\n",
    "                    num_replaced += 1\n",
    "                    self.L_vecs[i] = wv[word]\n",
    "                    self.R_vecs[i] = wv[word]\n",
    "            print (f'Replaced {float(num_replaced) / num_words}')\n",
    "            \n",
    "        self.L_vecs.requires_grad_()\n",
    "        self.R_vecs.requires_grad_()\n",
    "        \n",
    "        # gather the trainable parameters\n",
    "        self.parameters = [self.L_vecs, self.R_vecs]\n",
    "        \n",
    "    def forward(self, left_indices, right_indices):\n",
    "        \"\"\" Implement w_i^t w_j (the left-hand-side of Eq. (6) in the Glove paper)\n",
    "        \n",
    "            left_indices: torch.Tensor, a batch of center words\n",
    "            right_indices: torch.Tensor, a batch of context words, of the same shape of left_indices.\n",
    "            \n",
    "            left_indices[i] and right_indices[i] is the i-th pair in the training data.\n",
    "            \n",
    "            return: torch.Tensor of the same shape of left_indices\n",
    "        \"\"\"\n",
    "        ### Your codes go here (10 points) ###\n",
    "        X = []\n",
    "        for i in range(int(left_indices.size()[0])):\n",
    "          X.append(self.L_vecs[left_indices[i]].dot(self.R_vecs[right_indices[i]]))\n",
    "        \n",
    "        X = torch.DoubleTensor(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1goimxBiNRDi"
   },
   "source": [
    "## Model training, validating, and saving\n",
    "\n",
    "### First define some constants\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCvtEVViNF8D",
    "outputId": "52ea5bd5-3ff7-4f99-efaa-e5a605c4a90c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./glove.6B.300d.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# this will automatically place all tensor on GPU with type Double.\n",
    "# if you are not running on GPU, change this line to\n",
    "# torch.set_default_tensor_type('torch.DoubleTensor') GPU = torch.cuda.DoubleTensor\n",
    "torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
    "\n",
    "# set up a couple of parameters and hyper-parameters\n",
    "\n",
    "# number of epoches to train the model\n",
    "NUM_EPOCH = 25\n",
    "# size of mini-batches\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# dimension of word vectors. The integer should be the same as the dimension of\n",
    "# pretrained word vectors.\n",
    "NUM_DIMS = 300\n",
    "\n",
    "# how to many words to the right to pair with the center word\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "# two hyper-parameters in Eq. (9) of the paper\n",
    "x_max = 100\n",
    "alpha = 0.75\n",
    "\n",
    "# input file containing Amazon review texts.\n",
    "train_path = './small.csv'\n",
    "\n",
    "# where your model is saved.\n",
    "save_path = './glove_model_{}.pt'\n",
    "\n",
    "# optional word vectors pretrained\n",
    "pretrained_wv = './glove.6B.{}d.txt'.format(NUM_DIMS)\n",
    "print (pretrained_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "mn1Hz3QSqaCv"
   },
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "wv = load_pretrained_wv(pretrained_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnFqhAwurmCt",
    "outputId": "2896fd8e-2745-421a-ec94-208e3d7cd478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3602e-01, -1.1594e-01, -1.7078e-02, -2.9256e-01,  1.6149e-02,\n",
      "         8.6472e-02,  1.5759e-03,  3.4395e-01,  2.1661e-01, -2.1366e+00,\n",
      "         3.5278e-01, -2.3909e-01, -2.2174e-01,  3.6413e-01, -4.5021e-01,\n",
      "         1.2104e-01, -1.5596e-01, -3.8906e-02, -2.9419e-03,  1.6009e-02,\n",
      "        -1.1620e-01,  3.8680e-01,  3.5109e-01,  9.7426e-02, -1.2425e-02,\n",
      "        -1.7864e-01, -2.3259e-01, -2.6960e-01,  4.1083e-02, -7.6194e-02,\n",
      "        -2.3362e-01,  2.0919e-01, -2.7264e-01,  5.4967e-02, -1.8055e+00,\n",
      "         5.6348e-01, -1.2778e-01,  2.3147e-01, -5.8820e-03, -2.6630e-01,\n",
      "         4.1187e-01, -3.7162e-01, -2.0600e-01, -1.9619e-01, -4.3945e-03,\n",
      "         1.2513e-01,  4.6638e-01,  4.5159e-01, -1.5000e-01,  5.9589e-03,\n",
      "         5.9070e-02, -4.1440e-01,  6.1035e-02, -2.1117e-01, -4.0988e-01,\n",
      "         5.6393e-01,  2.3021e-01,  2.7240e-01,  4.9364e-02,  1.4239e-01,\n",
      "         4.1841e-01, -1.3983e-01,  3.4826e-01, -1.0745e-01, -2.5002e-01,\n",
      "        -3.2554e-01,  3.3343e-01, -3.5617e-01,  2.0442e-01,  1.4439e-01,\n",
      "        -1.2686e-01, -7.7273e-02, -1.9667e-01,  1.0759e-01, -1.1860e-01,\n",
      "        -2.5083e-01,  1.4205e-02,  2.7251e-01, -2.3707e-01, -2.3545e-01,\n",
      "        -1.5887e-01,  1.3151e-01,  6.9564e-01,  2.2766e-01,  1.8526e-01,\n",
      "         1.5743e-01, -1.5018e-01, -1.8177e-01, -3.3527e-02, -3.3092e-01,\n",
      "        -2.5205e-01,  5.0913e-01, -2.5607e-01, -5.3686e-01,  1.3397e-01,\n",
      "         6.7046e-02, -9.4473e-02, -2.2270e-01, -3.1469e-01,  8.5932e-02,\n",
      "        -4.3032e-02, -2.5821e-01, -9.5062e-02, -1.8497e-01,  5.8890e-02,\n",
      "         1.8972e-01, -1.7366e-01,  2.5263e-01, -5.4361e-01, -3.7248e-01,\n",
      "        -4.6661e-02, -4.1657e-01, -1.7549e-03, -4.8404e-01,  4.2090e-01,\n",
      "        -1.2749e-03,  9.4697e-03, -1.3380e-01,  7.2351e-02, -1.2096e-01,\n",
      "        -7.2870e-02, -1.8333e-01,  3.9652e-01,  1.1329e-01, -6.3029e-02,\n",
      "        -1.9702e-03,  4.2848e-01,  3.1790e-01, -1.5079e-01,  2.0405e-01,\n",
      "         2.1828e-01,  2.6067e-02,  4.3621e-02,  3.9224e-03, -2.6629e-01,\n",
      "        -2.8312e-01,  5.0497e-02, -1.8993e-01,  1.8996e-01,  2.9517e-01,\n",
      "        -1.1566e-01,  4.0967e-01,  2.2221e-01, -3.9778e-01, -3.3177e-01,\n",
      "        -1.3884e-01, -1.6829e-01, -2.0355e-01, -2.7687e-01, -1.1087e-01,\n",
      "        -6.7466e-01, -1.8108e-01,  1.8512e-01, -9.4616e-02,  1.7856e-01,\n",
      "        -6.6997e-02,  1.1379e-01, -9.3380e-02,  5.6860e-01, -1.3365e-01,\n",
      "         3.4636e-01, -4.1953e-01,  1.7547e-01, -2.4277e-02, -1.2441e-01,\n",
      "         9.2129e-02, -1.6702e-01, -1.4285e-01,  3.1646e-01,  3.0337e-01,\n",
      "         1.4840e-01, -6.7837e-03, -1.0509e+00,  2.2329e-01,  7.5211e-02,\n",
      "         4.4379e-02, -8.5929e-02, -1.1806e-01, -1.6632e-01, -7.8650e-02,\n",
      "         2.6374e-01, -2.2052e-01,  4.5582e-01, -1.5291e-01,  6.2617e-02,\n",
      "        -1.5588e-01,  8.2398e-02, -6.8462e-02, -2.4569e-01,  2.3439e-01,\n",
      "        -3.8633e-01,  2.4835e-01,  2.5334e-01, -2.1189e-01,  4.1494e-03,\n",
      "        -4.3762e-01, -1.3426e-01, -2.4583e-01,  1.4213e-01, -3.3973e-01,\n",
      "         1.4643e+00,  1.6414e-01,  2.2135e-01,  7.4099e-03, -5.5141e-02,\n",
      "        -2.7403e-02,  3.2928e-02,  1.4289e-01, -1.0049e-01, -2.2066e-01,\n",
      "        -3.0380e-01,  6.0624e-02, -1.2408e-01, -5.4114e-01,  2.4374e-01,\n",
      "         8.0903e-02, -7.8264e-02,  8.0091e-02,  9.8551e-03, -2.3077e-01,\n",
      "         1.6006e-01,  6.4075e-02, -4.1613e-01,  2.0494e-01, -1.8681e-01,\n",
      "         3.5367e-02,  2.1759e-01, -8.7823e-02,  3.5452e-01,  1.9578e-01,\n",
      "        -1.5127e-01, -1.0545e-01,  3.5650e-01, -3.8677e-01, -6.3172e-02,\n",
      "         3.1534e-01, -1.5887e-01, -3.1267e-01, -1.7893e-01,  4.1952e-01,\n",
      "         2.3261e-01,  2.0943e-01,  2.7013e-02,  1.7388e-02, -5.9857e-01,\n",
      "        -1.9622e-01, -2.3672e-01,  3.0032e-01,  4.6926e-02, -8.5768e-02,\n",
      "         3.6539e-01, -5.2476e-01, -1.3618e-01,  1.0868e-01,  4.6307e-01,\n",
      "         3.8502e-01,  7.6317e-04, -3.8196e-01,  7.9772e-02, -4.1744e-02,\n",
      "         4.7625e-02, -4.1018e-02,  1.7601e-01,  2.4893e-01, -1.0753e-01,\n",
      "         3.1935e-01, -1.2762e-01, -3.5059e-01,  3.5689e-04,  9.3515e-03,\n",
      "        -8.8616e-02, -3.2785e-01,  9.2063e-02, -6.1405e-02,  2.9053e-01,\n",
      "         2.2404e-02, -1.6879e+00,  2.6712e-01,  3.3419e-01, -5.2533e-02,\n",
      "        -1.9741e-01,  1.3709e-01, -5.4288e-02,  5.6423e-01,  1.9384e-01,\n",
      "         1.7229e-01,  2.9025e-01, -1.6124e-01,  5.9489e-02, -3.1884e-01,\n",
      "        -2.8343e-01,  6.4321e-02, -4.1589e-01, -7.0528e-02,  1.2410e-02,\n",
      "        -4.0208e-01, -2.4963e-01, -3.3760e-01,  7.0098e-02,  2.4642e-01],\n",
      "       device='cpu')\n"
     ]
    }
   ],
   "source": [
    "print (wv['good'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z39JUijCxi4i"
   },
   "source": [
    "### Then define the training, validation, and test data.\n",
    "*   Use the AmazonReviewGloveDataset class to read train dataset.\n",
    "*   Define DataLoader wrapping around the Dataset objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A77sOk99H-nv",
    "outputId": "435e7017-374e-499f-c64e-28d8650e2f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99946 reviews loaded. 493526 sentences.\n",
      "Indexing the corpus...\n",
      "Tokenizing sentences into words...\n",
      "Done implementing tokenization and taking out stopwords.\n",
      "Building a dictionary with occurrences of words...\n",
      "Removing words that don't meet the minimum word occurrences...\n",
      "Vocabulary in word_to_index and index_to_word.\n",
      "Done indexing the corpus.\n",
      "Constructing the co-occurrence matrix...\n",
      "Done constructing the co-occurrence matrix.\n",
      "Number of entries =  7133935\n"
     ]
    }
   ],
   "source": [
    "# load text data and turn them into a DataLoader object.\n",
    "train_dataset = AmazonReviewGloveDataset(train_path, right_window = WINDOW_SIZE)\n",
    "train_iter = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJE0ElajbgIA",
    "outputId": "f8dc7d28-4890-4473-be80-332659f4b05f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000011D4C6E8DC0>\n",
      "[350, 350, 244, 1520, 1520, 1520, 1520, 355, 355, 1539, 4526, 4526, 2351, 2351, 2351, 12055, 12055, 12055, 3072]\n",
      "(1, 3, 8)\n"
     ]
    }
   ],
   "source": [
    "print(train_iter)\n",
    "print(train_dataset.left[len(train_dataset.left)-20:len(train_dataset.left)-1])\n",
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFfzclO7yOwU"
   },
   "source": [
    "### Third, start training.\n",
    "\n",
    "*   You're required to use GPU to train the network, since GPU are ubiquitous (colab or SandBox).\n",
    "*   Complete the function train_and_validate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBqcAinANLUn",
    "outputId": "f7357486-80ac-4973-9156-191e0eebe6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 0.8538461538461538\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# decide whether to use cpu or gpu\n",
    "device = torch.device('cuda')\n",
    "# initialize the Glove model\n",
    "model = GloveModel(train_dataset.indexer, wv, word_dims = NUM_DIMS)\n",
    "\n",
    "# make sure you use weight_decay to activate the L2 regularization\n",
    "optimizer = torch.optim.Adam(model.parameters, weight_decay=1e-8)\n",
    "\n",
    "def train_and_validate(train_iter):\n",
    "    best_loss = -1\n",
    "    best_epoch = -1\n",
    "    to_save = {}\n",
    "    \n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(train_iter)\n",
    "        it = 0\n",
    "        for l, r, n_lr in train_iter:\n",
    "            it += 1\n",
    "            optimizer.zero_grad()\n",
    "            # Implement the loss function in Eq. (16) of the paper, in three steps.\n",
    "            # Step 1. find the prediction of log(X_ij) using the model \n",
    "            ### Your codes go here (3 points) ###\n",
    "            pred = model.forward(l,r)\n",
    "            # Step 2. compute the weights f(X_ij). See Eq. (9) of the Glove paper.\n",
    "            ### Your codes go here (3 points) ###\n",
    "            weights = []\n",
    "            i = 0;\n",
    "            for x_ij in n_lr:\n",
    "              pred[i] = pred[i] - math.log(x_ij,10)\n",
    "              i += 1\n",
    "              if x_ij < x_max: \n",
    "                weight = (x_ij/x_max)**alpha\n",
    "              else:\n",
    "                weight = 1\n",
    "              weights.append(weight)\n",
    "            weights = torch.DoubleTensor(weights)\n",
    "            # Step 3. compute the loss in Eq. (16) using the predictions and the weights\n",
    "            ### Your codes go here (4 points) ###\n",
    "            loss = 0\n",
    "            for i in range(int(pred.size()[0])):\n",
    "              loss += weights[i] * (pred[i]** 2) #where weight = f(x_ij) \n",
    "\n",
    "            var_loss = Variable(loss.data, requires_grad=True)\n",
    "            # tracking the averaged loss\n",
    "            epoch_loss += loss.item()\n",
    "            #print(\"Epoch Loss: \", epoch_loss, \"Iteration: \", it)\n",
    "            # gradient descent, don't change the following two lines\n",
    "            var_loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Training epoch = {epoch}, epoch loss = {epoch_loss / num_batches}')\n",
    "\n",
    "        # record the model state_dict() for saving later\n",
    "        #to_save = {\n",
    "         #   'epoch': epoch,\n",
    "          #  'model_state_dict': model.state_dict()\n",
    "        #}\n",
    "        #torch.save(to_save, save_path.format(epoch))\n",
    "        #print (save_path.format(epoch))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "NI1ZaELfXWFe",
    "outputId": "426b72fa-425d-4cc6-d8f4-0457728aa63e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch = 0, epoch loss = 2008.167551130877\n",
      "Training epoch = 1, epoch loss = 2008.167551130886\n",
      "Training epoch = 2, epoch loss = 2008.167551130879\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-56cf34be9e8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_and_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-107-f7ffc6e2c706>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[1;34m(train_iter)\u001b[0m\n\u001b[0;32m     34\u001b[0m               \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_ij\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m               \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m               \u001b[1;32mif\u001b[0m \u001b[0mx_ij\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mx_max\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_ij\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_and_validate(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPAs6x2kqUMY"
   },
   "source": [
    "## Retrieve similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWTYckiiqT3n"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import no4\n",
    "rmalize\n",
    "\n",
    "test_aspect_words = ['phone', 'case', 'battery', 'headset', 'charger', 'quality', 'screen', 'bluetooth', 'price', 'device']\n",
    "test_sentimental_words = ['great', 'good', 'well', 'works', 'better', 'little', 'easy', 'nice', 'new', 'long']\n",
    "\n",
    "glove = load_model(save_path.format(0), train_dataset.indexer)\n",
    "avg_word_vectors = (glove.L_vecs.to('cpu') + glove.R_vecs.to('cpu')) / 2\n",
    "avg_word_vectors = avg_word_vectors.detach().numpy()\n",
    "\n",
    "n_words = train_dataset.indexer.n_words\n",
    "\n",
    "row_normalized = normalize(avg_word_vectors)\n",
    "sim = row_normalized.dot(row_normalized.T)\n",
    "\n",
    "for w in test_aspect_words:\n",
    "    w_idx = train_dataset.indexer.word_to_index[w]\n",
    "    l = []\n",
    "    for i in range(n_words):\n",
    "        l.append((i, sim[w_idx, i]))\n",
    "    l = sorted(l, key = lambda x:x[1], reverse = True)\n",
    "    for i in range(10):\n",
    "        print (f'{train_dataset.indexer.index_to_word[l[i][0]]}: {l[i][1]}')\n",
    "        \n",
    "for w in test_sentimental_words:\n",
    "    w_idx = train_dataset.indexer.word_to_index[w]\n",
    "    l = []\n",
    "    for i in range(n_words):\n",
    "        l.append((i, sim[w_idx, i]))\n",
    "    l = sorted(l, key = lambda x:x[1], reverse = True)\n",
    "    for i in range(10):\n",
    "        print (f'{train_dataset.indexer.index_to_word[l[i][0]]}: {l[i][1]}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Glove_release.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
